{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\" –í –æ—Å—Ç—Ä–æ–≤–Ω–æ–º –∫–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–µ –ú–µ–ª–Ω–∏–±–æ–Ω—ç –∏—Å—Ç–æ–≤–æ –±–ª—é–ª–∏ –¥—Ä–µ–≤–Ω–∏–µ –æ–±—ã—á–∞–∏, —Ö–æ—Ç—è –≤–æ—Ç —É–∂–µ –ø—è—Ç—å—Å–æ—Ç –ª–µ—Ç –¥–µ–Ω—å –∑–∞ –¥–Ω–µ–º –∏ —á–∞—Å –∑–∞ —á–∞—Å–æ–º —Ç–µ—Ä—è–ª–∞ –¥–µ—Ä–∂–∞–≤–∞ —Å–≤–æ–µ –±—ã–ª–æ–µ –º–æ–≥—É—â–µ—Å—Ç–≤–æ –∏ –¥–µ—Ä–∂–∞–ª–∞—Å—å —Ç–µ–ø–µ—Ä—å –ª–∏—à—å —Ç–æ—Ä–≥–æ–≤–ª–µ–π —Å –ú–æ–ª–æ–¥—ã–º–∏ –ö–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–∞–º–∏, –¥–∞ –ø–æ—Ç–æ–º—É –µ—â–µ, —á—Ç–æ –ü—Ä–∏–∑—Ä–∞—á–Ω—ã–π –ì–æ—Ä–æ–¥ –ò–º—Ä—Ä–∏—Ä —Å–ª—É–∂–∏–ª –º–µ—Å—Ç–æ–º –≤—Å—Ç—Ä–µ—á–∏ –∫—É–ø—Ü–∞–º —Å–æ –≤—Å–µ–≥–æ —Å–≤–µ—Ç–∞. –ú–Ω–æ–≥–æ –ª–∏ –æ—Ç —Ç–µ—Ö –æ–±—ã—á–∞–µ–≤ –ø–æ–ª—å–∑—ã? –ù–µ–ª—å–∑—è –ª–∏ –æ—Ç—Ä–∏–Ω—É—Ç—å –∏—Ö –∏ —Ç–µ–º —Å–∞–º—ã–º –∏–∑–±–µ–∂–∞—Ç—å —É–≥–æ—Ç–æ–≤–∞–Ω–Ω–æ–π —Å—É–¥—å–±—ã?\n",
    "–°–æ–ø–µ—Ä–Ω–∏–∫ –∏–º–ø–µ—Ä–∞—Ç–æ—Ä–∞ –≠–ª—Ä–∏–∫–∞ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–ª –Ω–µ –∑–∞–¥–∞–≤–∞—Ç—å—Å—è —Ç–∞–∫–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –û–Ω –∑–∞—è–≤–ª—è–ª –≤–æ –≤—Å–µ—É—Å–ª—ã—à–∞–Ω—å–µ, —á—Ç–æ, –ø—Ä–µ–Ω–µ–±—Ä–µ–≥–∞—è –æ–±—ã—á–∞—è–º–∏ –ø—Ä–µ–¥–∫–æ–≤, –≠–ª—Ä–∏–∫ –Ω–∞–≤–ª–µ—á–µ—Ç –≥–∏–±–µ–ª—å –Ω–∞ –ú–µ–ª–Ω–∏–±–æ–Ω—ç.\n",
    "–í–æ—Ç –Ω–∞—á–∞–ª–æ —Ç–æ–π —Ç—Ä–∞–≥–∏—á–µ—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –¥–ª–∏–ª–∞—Å—å –Ω–µ –≥–æ–¥ –∏ –Ω–µ –¥–≤–∞ –∏ –∑–∞ –∫–æ–Ω—Ü–æ–º –∫–æ—Ç–æ—Ä–æ–π ‚Äî –≥–∏–±–µ–ª—å –≤—Å–µ–≥–æ —ç—Ç–æ–≥–æ –º–∏—Ä–∞.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': '–ù–µ  —Ç–µ—Ä—è–ª–∞ –¥–µ—Ä–∂–∞–≤–∞   \\xa0‚Äò–ø–µ—Ä–∞—Ç–æ—Ä–∞‚Äô,   ‚Äò‚Äò‚Äô–ò\\u2009‚Äô‚Äô  ‚Äú‚Äù‚Äô ‚Äò–ù.‚Äù  ‚Äù‚Äù,  ‚Äô‚Äù. ‚Äù, ‚Äù ‚Äò, ‚Äô, \\u2009‚Äù,. \\u2009, ‚Äò.‚Äô. \\u2009. ‚Äú–ù\\u2009 ‚ÄúI‚Äôm sorry,‚Äù ‚Äú'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–æ–±—É—á–∏–º "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b374266bb74f13a273c7d6d92baae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8b8fb0620043958b149e43fe6d4048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed2ddb2083b4af8b1374cdffd653d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 844.175, 'train_samples_per_second': 0.018, 'train_steps_per_second': 0.009, 'train_loss': 4.095225811004639, 'epoch': 1.0}\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.\n",
      "–ü–µ—Ä–µ—Å–∫–∞–∑: –ù–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –æ—â–µ—Ä–∏–π —Ç–µ—Ä–æ–π –ø—Ä\n",
      "\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\n",
      "–ü–µ—Ä–µ—Å–∫–∞–∑:  –æ–±—Ä–∞–ø–µ—Ö–æ–π –ø—Ä–µ–≥—Ä–∞—â–µ–Ω –æ—Ç –æ –ø–µ—Ä–ø—Ä–∞–º–µ–Ω—Ç–µ–π –≥–æ—Ä–æ–π —Ä–∞–∫–æ–Ω –ø–æ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import random\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (2% –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞)\n",
    "def load_sample_data(file_path, fraction=0.02):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return random.sample(lines, max(1, int(len(lines) * fraction)))\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "train_samples = load_sample_data('train_text_ru.txt')\n",
    "val_samples = load_sample_data('val_text_ru.txt')\n",
    "\n",
    "def prepare_samples(samples):\n",
    "    sources, targets = [], []\n",
    "    for line in samples:\n",
    "        parts = line.strip().split('</s>')\n",
    "        if len(parts) >= 2:\n",
    "            sources.append(parts[0].strip())\n",
    "            targets.append(parts[1].replace('<s>', '').replace('</s>?', '').strip())\n",
    "    return {'source': sources, 'target': targets}\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict(prepare_samples(train_samples)),\n",
    "    'validation': Dataset.from_dict(prepare_samples(val_samples))\n",
    "})\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(examples['source'], max_length=128, truncation=True, padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target'], max_length=64, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = dataset.map(tokenize_fn, batched=True, batch_size=8)\n",
    "\n",
    "# 4. –û–±–ª–µ–≥—á–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,  # –í—Å–µ–≥–æ 1 —ç–ø–æ—Ö–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")\n",
    "\n",
    "# 5. –ö—Ä–∞—Ç–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ (—Ç–æ–ª—å–∫–æ 1 —ç–ø–æ—Ö–∞)\n",
    "trainer.train()\n",
    "\n",
    "# 6. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "test_texts = [\n",
    "    \"–ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.\",\n",
    "    \"–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\"\n",
    "]\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Å–∫–∞–∑–æ–≤\n",
    "for text in test_texts:\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ü–µ—Ä–µ—Å–∫–∞–∑: {summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑—å–º–µ–º –¥–≤–µ —ç–ø–æ—Ö–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316400c6809a4213aa0097eb61661efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8498805ea54427ac7e02266540234e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892de51e57a440798e90b0a766e03de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicks\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 809.0544, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.017, 'train_loss': 3.604740687779018, 'epoch': 2.0}\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.\n",
      "–ü–µ—Ä–µ—Å–∫–∞–∑: –ù–∞ –¥–µ—Ä–æ–π –ø–æ–ª–µ–∂–∏—Ç—å –Ω–µ –ø–æ—â–µ –Ω–∞ –≥–æ—Ä–æ–Ω–∏—è –æ–≥—Ä–æ–ø–æ—Ä–µ–Ω–∏—â–∏—á–∏ –ø—Ä–æÔøΩ\n",
      "\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\n",
      "–ü–µ—Ä–µ—Å–∫–∞–∑: –ø—Ä–µ—â–µ–Ω –æ–≥—Ä–µ–Ω–∏—è –Ω–µ–≥–æ—Ä–æ–ª–µ –Ω–∞–∂–µ –¥–µ–π—Å—Ç–∞—Ç—å –ø–æ –ø—Ä–æ—á–∫–∏ –¥–æ–ª–∂–∞\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import random\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (2% –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞)\n",
    "def load_sample_data(file_path, fraction=0.02):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return random.sample(lines, max(1, int(len(lines) * fraction)))\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "train_samples = load_sample_data('train_text_ru.txt')\n",
    "val_samples = load_sample_data('val_text_ru.txt')\n",
    "\n",
    "def prepare_samples(samples):\n",
    "    sources, targets = [], []\n",
    "    for line in samples:\n",
    "        parts = line.strip().split('</s>')\n",
    "        if len(parts) >= 2:\n",
    "            sources.append(parts[0].strip())\n",
    "            targets.append(parts[1].replace('<s>', '').replace('</s>?', '').strip())\n",
    "    return {'source': sources, 'target': targets}\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict(prepare_samples(train_samples)),\n",
    "    'validation': Dataset.from_dict(prepare_samples(val_samples))\n",
    "})\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(examples['source'], max_length=128, truncation=True, padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target'], max_length=64, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = dataset.map(tokenize_fn, batched=True, batch_size=8)\n",
    "\n",
    "# 4. –û–±–ª–µ–≥—á–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (2 —ç–ø–æ—Ö–∏)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,  # –¢–µ–ø–µ—Ä—å 2 —ç–ø–æ—Ö–∏\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")\n",
    "\n",
    "# 5. –û–±—É—á–µ–Ω–∏–µ (2 —ç–ø–æ—Ö–∏)\n",
    "trainer.train()\n",
    "\n",
    "# 6. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "test_texts = [\n",
    "    \"–ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.\",\n",
    "    \"–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\"\n",
    "]\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Å–∫–∞–∑–æ–≤\n",
    "for text in test_texts:\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ü–µ—Ä–µ—Å–∫–∞–∑: {summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:  –í –æ—Å—Ç—Ä–æ–≤–Ω–æ–º –∫–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–µ –ú–µ–ª–Ω–∏–±–æ–Ω—ç –∏—Å—Ç–æ–≤–æ –±–ª—é–ª–∏ –¥—Ä–µ–≤–Ω–∏–µ –æ–±—ã—á–∞–∏, —Ö–æ—Ç—è –≤–æ—Ç —É–∂–µ –ø—è—Ç—å—Å–æ—Ç –ª–µ—Ç –¥–µ–Ω—å –∑–∞ –¥–Ω–µ–º –∏ —á–∞—Å –∑–∞ —á–∞—Å–æ–º —Ç–µ—Ä—è–ª–∞ –¥–µ—Ä–∂–∞–≤–∞ —Å–≤–æ–µ –±—ã–ª–æ–µ –º–æ–≥—É—â–µ—Å—Ç–≤–æ –∏ –¥–µ—Ä–∂–∞–ª–∞—Å—å —Ç–µ–ø–µ—Ä—å –ª–∏—à—å —Ç–æ—Ä–≥–æ–≤–ª–µ–π —Å –ú–æ–ª–æ–¥—ã–º–∏ –ö–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–∞–º–∏, –¥–∞ –ø–æ—Ç–æ–º—É –µ—â–µ, —á—Ç–æ –ü—Ä–∏–∑—Ä–∞—á–Ω—ã–π –ì–æ—Ä–æ–¥ –ò–º—Ä—Ä–∏—Ä —Å–ª—É–∂–∏–ª –º–µ—Å—Ç–æ–º –≤—Å—Ç—Ä–µ—á–∏ –∫—É–ø—Ü–∞–º —Å–æ –≤—Å–µ–≥–æ —Å–≤–µ—Ç–∞. –ú–Ω–æ–≥–æ –ª–∏ –æ—Ç —Ç–µ—Ö –æ–±—ã—á–∞–µ–≤ –ø–æ–ª—å–∑—ã? –ù–µ–ª—å–∑—è –ª–∏ –æ—Ç—Ä–∏–Ω—É—Ç—å –∏—Ö –∏ —Ç–µ–º —Å–∞–º—ã–º –∏–∑–±–µ–∂–∞—Ç—å —É–≥–æ—Ç–æ–≤–∞–Ω–Ω–æ–π —Å—É–¥—å–±—ã?\n",
      "–°–æ–ø–µ—Ä–Ω–∏–∫ –∏–º–ø–µ—Ä–∞—Ç–æ—Ä–∞ –≠–ª—Ä–∏–∫–∞ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–ª –Ω–µ –∑–∞–¥–∞–≤–∞—Ç—å—Å—è —Ç–∞–∫–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –û–Ω –∑–∞—è–≤–ª—è–ª –≤–æ –≤—Å–µ—É—Å–ª—ã—à–∞–Ω—å–µ, —á—Ç–æ, –ø—Ä–µ–Ω–µ–±—Ä–µ–≥–∞—è –æ–±—ã—á–∞—è–º–∏ –ø—Ä–µ–¥–∫–æ–≤, –≠–ª—Ä–∏–∫ –Ω–∞–≤–ª–µ—á–µ—Ç –≥–∏–±–µ–ª—å –Ω–∞ –ú–µ–ª–Ω–∏–±–æ–Ω—ç.\n",
      "–í–æ—Ç –Ω–∞—á–∞–ª–æ —Ç–æ–π —Ç—Ä–∞–≥–∏—á–µ—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –¥–ª–∏–ª–∞—Å—å –Ω–µ –≥–æ–¥ –∏ –Ω–µ –¥–≤–∞ –∏ –∑–∞ –∫–æ–Ω—Ü–æ–º –∫–æ—Ç–æ—Ä–æ–π ‚Äî –≥–∏–±–µ–ª—å –≤—Å–µ–≥–æ —ç—Ç–æ–≥–æ –º–∏—Ä–∞.\n",
      "\n",
      "–ü–µ—Ä–µ—Å–∫–∞–∑: –ö–∞–≥–æ –æ–≥—Ä–æ–¥–µ –Ω–µ–≥–æ–ª–∞–Ω–∏—á–µ–Ω–∏–µ –¥–µ–Ω—å –∑–∞ –¥–Ω–µ–º –∏ —á–∞—Å –∑–µ –ø–µ—Ä–µ–∂–µ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"\"\" –í –æ—Å—Ç—Ä–æ–≤–Ω–æ–º –∫–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–µ –ú–µ–ª–Ω–∏–±–æ–Ω—ç –∏—Å—Ç–æ–≤–æ –±–ª—é–ª–∏ –¥—Ä–µ–≤–Ω–∏–µ –æ–±—ã—á–∞–∏, —Ö–æ—Ç—è –≤–æ—Ç —É–∂–µ –ø—è—Ç—å—Å–æ—Ç –ª–µ—Ç –¥–µ–Ω—å –∑–∞ –¥–Ω–µ–º –∏ —á–∞—Å –∑–∞ —á–∞—Å–æ–º —Ç–µ—Ä—è–ª–∞ –¥–µ—Ä–∂–∞–≤–∞ —Å–≤–æ–µ –±—ã–ª–æ–µ –º–æ–≥—É—â–µ—Å—Ç–≤–æ –∏ –¥–µ—Ä–∂–∞–ª–∞—Å—å —Ç–µ–ø–µ—Ä—å –ª–∏—à—å —Ç–æ—Ä–≥–æ–≤–ª–µ–π —Å –ú–æ–ª–æ–¥—ã–º–∏ –ö–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–∞–º–∏, –¥–∞ –ø–æ—Ç–æ–º—É –µ—â–µ, —á—Ç–æ –ü—Ä–∏–∑—Ä–∞—á–Ω—ã–π –ì–æ—Ä–æ–¥ –ò–º—Ä—Ä–∏—Ä —Å–ª—É–∂–∏–ª –º–µ—Å—Ç–æ–º –≤—Å—Ç—Ä–µ—á–∏ –∫—É–ø—Ü–∞–º —Å–æ –≤—Å–µ–≥–æ —Å–≤–µ—Ç–∞. –ú–Ω–æ–≥–æ –ª–∏ –æ—Ç —Ç–µ—Ö –æ–±—ã—á–∞–µ–≤ –ø–æ–ª—å–∑—ã? –ù–µ–ª—å–∑—è –ª–∏ –æ—Ç—Ä–∏–Ω—É—Ç—å –∏—Ö –∏ —Ç–µ–º —Å–∞–º—ã–º –∏–∑–±–µ–∂–∞—Ç—å —É–≥–æ—Ç–æ–≤–∞–Ω–Ω–æ–π —Å—É–¥—å–±—ã?\n",
    "–°–æ–ø–µ—Ä–Ω–∏–∫ –∏–º–ø–µ—Ä–∞—Ç–æ—Ä–∞ –≠–ª—Ä–∏–∫–∞ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–ª –Ω–µ –∑–∞–¥–∞–≤–∞—Ç—å—Å—è —Ç–∞–∫–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –û–Ω –∑–∞—è–≤–ª—è–ª –≤–æ –≤—Å–µ—É—Å–ª—ã—à–∞–Ω—å–µ, —á—Ç–æ, –ø—Ä–µ–Ω–µ–±—Ä–µ–≥–∞—è –æ–±—ã—á–∞—è–º–∏ –ø—Ä–µ–¥–∫–æ–≤, –≠–ª—Ä–∏–∫ –Ω–∞–≤–ª–µ—á–µ—Ç –≥–∏–±–µ–ª—å –Ω–∞ –ú–µ–ª–Ω–∏–±–æ–Ω—ç.\n",
    "–í–æ—Ç –Ω–∞—á–∞–ª–æ —Ç–æ–π —Ç—Ä–∞–≥–∏—á–µ—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –¥–ª–∏–ª–∞—Å—å –Ω–µ –≥–æ–¥ –∏ –Ω–µ –¥–≤–∞ –∏ –∑–∞ –∫–æ–Ω—Ü–æ–º –∫–æ—Ç–æ—Ä–æ–π ‚Äî –≥–∏–±–µ–ª—å –≤—Å–µ–≥–æ —ç—Ç–æ–≥–æ –º–∏—Ä–∞.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Å–∫–∞–∑–æ–≤\n",
    "for text in test_texts:\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ü–µ—Ä–µ—Å–∫–∞–∑: {summary}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
